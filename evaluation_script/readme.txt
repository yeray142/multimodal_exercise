The evaluation script will load 'predictions_test_set.csv' and 'test_set_age_labels.csv' and compute a set of values (e.g., global average accuracy, average accuracy given different attributes, and some bias metrics).

The 'predictions_test_set.csv' file is generated by the baseline model, when 'batch_size_test = 1'. In case you generate this file in a different way, you will need to adapt the evaluation script to load it properly.

The easiest way to use the evaluation script, is to generate the 'predictions_test_set.csv' file as it is, but changing the values of the 'prediction' column by the ones estimated by your model.

Then, you can simply run 'python evaluate.py'